<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Embodied-AI-with-DP-and-LLM</title>
    <link rel="stylesheet" href="https://rt-hierarchy.github.io/css/bootstrap.min.css" >
</head>
<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+8">Embodied-AI-with-DP-and-LLM</font></strong> </br> 
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center authors">
                <ul class="list-inline">
                <br>
                <li>
                    <a href="https://github.com/yue-zhao-robot?tab=repositories">Yue Zhao</a><sup>*,1</sup>, Xiaozhu Ju</a><sup>*,1</sup>, Zhihang Li</a><sup>*,2</sup>, Xuelian Geng</a><sup>2</sup>, Zizhuang Guo</a><sup>3</sup>,
                    <!-- <a href="https://research.google/people/tianli-ding/">Tianli Ding</a><sup>1</sup>,  -->

                </li>
                <br>
                </ul>
                <sup>1</sup>Innovation Center of Beijing
                Embodied Artificial Intelligence Robot, <sup>2</sup>Beijing Jiaotong University,<sup>3</sup>Johns Hopkins University
            </div>
            <br>
        </div>

        <div class="row">
            <div class="col-md-12">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a src="data/root.pdf" target="_blank">
                        <image src="data/paper.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>


        <br>

        <div class="row" style="width: 80%; margin: 0 auto;"> 
            <div class="col-md-12">
                <h3 class="col-md-12 text-center"><strong>Abstract</strong></h3>
                <p>
                    The development of large language model
                    (LLM) and visual language model(VLM) has driven rapid
                    progress in embodied artificial intelligence (Embodied AI)
                    technology, opening up new avenues for robots to interact
                    with the physical world and providing vast space for robots
                    to perform more complex manipulation tasks. The diffusion
                    policy does not rely on the robot dynamics model and can
                    elegantly handle multimodal action distributions, it’s suitable
                    for high-dimensional action spaces, and exhibits impressive
                    training stability, making it highly suitable for robot precise
                    control task. However, the diffusion policy cannot interact
                    with the physical world and is difficult to guarantee the
                    generalization for different task scenarios. To address these
                    issues, this article proposes an Embodied AI scheme based
                    on the LLM and the diffusion policy. This solution firstly
                    converts human natural language instructions into robot
                    manipulation subtasks by LLM and then generates robot
                    actions based on the diffusion policy. Compared to existing
                    Embodied AI solutions, the novelty of our method lies in two
                    aspects: even when facing complex human instructions, it only
                    needs to call the LLM once, reducing the time consumption
                    of frequently calling the LLM. And, we increase the accuracy
                    of task execution by suppressing the states of objects that
                    we are not interested in while enhancing the states of task
                    objects to guide the specific behavioral actions of robots. To
                    validate our idea, we conducted simulation experiments on a
                    7 dof Franka Emika Panda using Isaac Sim. The experimental
                    results show the correctness and generalization of our method.
                </p>
<!--                 <ul class="nav nav-pills nav-justified"> -->
                 <h3 class="col-md-12 text-center"><strong>The Overall Framework of our Embodied Artificial Intelligent Scheme</strong></h3>
                    <li>
                        <a src="data/overall.jpg" target="_blank">
                        <image src="data/overall.jpg" height="300px">
                            <!-- <h4><strong>Paper</strong></h4> -->
                        </a>
                    </li>
                </ul>
            </div>
        </div>
        <br>
        
        <h3 class="col-md-12 text-center"><strong>Video</strong></h3>

        <!-- <p style="text-align:center; width: 80%; margin: 0 auto">
            <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                <source src="data/Screencast.webm" type="video/webm">
                Your browser does not support the video tag.
            </video>
        </p> -->


        <div style="width: 100%; margin: 0 auto">
            <div style="width: 20%; float: left; text-align:center; margin: 0 auto">
                <!-- <a src="data/method0.jpg" target="_blank"> -->
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                        <source src="data/2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <figcaption>fig 1
                    </figcaption>
                </a>
            </div>
            <div style="width: 20%; float: right; text-align:center; margin: 0 auto">
                <!-- <a src="data/method1.png" target="_blank"> -->
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                        <source src="data/2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <figcaption>fig 2
                    </figcaption>
                </a>
            </div>
            <div style="width: 20%; float: right; text-align:center; margin: 0 auto">
                <!-- <a src="data/method1.png" target="_blank"> -->
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                        <source src="data/2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>fig 3
                    </figcaption>
                </a>
            </div>
            <div style="width: 20%; float: right; text-align:center; margin: 0 auto">
                <!-- <a src="data/method1.png" target="_blank"> -->
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                        <source src="data/2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <figcaption>fig 4
                    </figcaption>
                </a>
            </div>
            <div style="width: 20%; float: right; text-align:center; margin: 0 auto">
                <!-- <a src="data/method1.png" target="_blank"> -->
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                        <source src="data/2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <figcaption>fig 5
                    </figcaption>
                </a>
            </div>
        </div>

        <br>
        <hr class="section-divider">

        <div class="row">
            <div class="col-md-12">
                <h3 class="col-md-12 text-center"><strong>Method</strong></h3>
                <p>
                    The implementation process of our Embodied AI scheme is shown in Algorithm 1.
                </p>
                <div style="width: 70%; margin: 0 auto">
                    <div style="width: 35%; float: left; text-align:center; margin: 0 auto">
                        <!-- <a src="data/method0.jpg" target="_blank"> -->
                            <figure>
                                <img src="data/2.jpg" class="img-responsive">
                            </figure>
                        </a>
                        <!-- <figcaption>XXXXXX</figcaption> -->
                    </div>
                    <div style="width: 50%; float: right; text-align:center; margin: 0 auto">
                        <!-- <a src="data/method1.png" target="_blank"> -->
                            <figure>
                                <img src="data/alg2.png" class="img-responsive">
                            </figure>
                        </a>
                        <!-- <figcaption>XXXXXXX</figcaption> -->
                    </div>
                </div>
                <br>
                <br>
                <br>
                <br>

            </div>
        </div>
        <p>In Algorithm 1, based on the environmental informa-
            tion collected by the RGB-D depth camera, LLM is first
            used to decompose human natural language instructions
            into several sub-tasks that can be executed by robots
            according to the designed prompt template. Then, a
            dataset combined states with actions is obtained in
            NVIDIA Isaac Sim. Based on this dataset, a diffusion
            model is trained to approximate the trajectory distri-
            bution. Then, based on the diffusion model, a reverse
            denoising process is carried out, ultimately obtaining the
            robot trajectory action..</p>

        <div class="row">
            <div class="col-md-12">
                <h3 ><strong><font size="+3">Experiments</font></strong></h3>
                <h4><strong>A:Efficient Analysis for Manipulation Tasks</strong></h4>
                <!-- Training -->
                <br>
                <!-- <h4 class="text-center"><strong>Training on Diverse Tasks</strong></h4> -->
                <br>
                <div style="width: 100%; margin: 0 auto">
                    <div style="width: 70%; float: left; text-align:center; margin: 0 auto">
                        <a src="data/method0.jpg" target="_blank">
                            <figure>
                                <img src="data/3.jpg" class="img-responsive">
                            </figure>
                            <figcaption>Task scenario and trajectory visualization
                            </figcaption>
                        </a>
                    </div>
                </div>
                <br>
            </div>
            <p> We designed a series of manipulation tasks in the
                simulation environment to verify whether the proposed
                algorithm can accurately plan complete and feasible
                trajectories for known or similar tasks during the training
                process. We demonstrate three representative tasks as
                shown in Figure 3, which visualizes the scene information
                and the trajectory data of the planned terminal effector. </p>
        </div>

        <hr class="section-divider">

        <div class="row col-md-12">

            <!--  -->
            <h4><strong>B: Generalization to Complex Task</strong></h4>
            <br>

            <!-- <h4 class="text-center"><strong>New Scenes</strong></h4> -->
            <br>
            <div style="width: 100%; margin: 0 auto">
                <div style="width: 70%; float: left; text-align:center; margin: 0 auto">
                    <a src="data/4.jpg" target="_blank">
                        <figure>
                            <img src="data/4.jpg" class="img-responsive">
                        </figure>
                        <figcaption>The success rate of 3 types of tasks
                        </figcaption>
                    </a>
                </div>
            </div>
            <br>
        </div>
            <p> In table II, the UI represents unknown instructions,
                while SA and UA represent scene attributes that have
                been seen or unseen. Our model performs well on
                different instructions and scene attributes. The classification success rates are shown in Table 2.
                The experimental results show that our model has good
                generalization for all scenarios. For long-term planning
                tasks, the high success rate is mainly due to the model’s
                trajectory combination ability. For example, for the
                block stacking task, our model first breaks it down into
                multiple simple subtasks and then executes each subtask
                sequentially to reduce the difficulty of the task. For
                simple spatial combination tasks, thanks to the language
                instruction mapping behavior of LLM, a high success rate
                can be maintained regardless of whether the scene objects
                change. Whether fuzzy instruction tasks are completed
                depends on the language understanding ability of the
                large model and the model’s degree of cognition of
                scene objects. Therefore, when the scene prior is lost,
                its success rate will be significantly reduced. Comparing
                spatial combination tasks with abstract semantic tasks,
                we found that using LLM to map natural language into
                a ”language” that the robot can understand, such as the
                environment state si , is more flexible and generalized
                than directly defining language behavior instructions. </p>
            </div>
            <hr class="section-divider">
            <h4><strong>C: Real-time Planning for Dynamic Target</strong></h4>
            <br>
            <div style="width: 100%; margin: 0 auto">
                <div style="width: 70%; float: left; text-align:center; margin: 0 auto">
                    <a src="data/5.jpg" target="_blank">
                        <figure>
                            <img src="data/5.jpg" class="img-responsive">
                        </figure>
                    </a>
                </div>
            </div>
            <br>
        </div>
            <p>
                We verify the real-time trajectory plan
                ning capability of our model by introducing displacement
                properties in the scene, that is, changing the scene
                from static to dynamic. Compared with the traditional
                control model, the diffusion model naturally has the
                properties of observation-movement, re-observation-re-
                movement, that is, its behavior at time t+1 is only
                determined by the state observation and action at time t,
                which makes trajectory error correction and re-planning
                possible. We conducted experiments on the three types
                of tasks described in A:Task scenario and trajectory visualization. We set a speed
                attribute for the target object of the task so that it can
                move forward at a certain speed in a random direction,
                ensuring that the target position attribute queried by
                LLM keeps changing. The planned trajectory is shown in
                Figure above. By observing the trajectory, it is not difficult to
                find that when the target position changes significantly,
                the generated trajectory will change in curvature, which
                triggers the trajectory correction of the model. Therefore,
                our model has a strong re-planning ability and is more
                robust to small disturbances in the scene.
            </p>
        </div>
            <div class="row">
                <div class="col-md-12">
                    <h3 ><strong><font size="+3">Prompts</font></strong></h3>
                    <h4><font size="+1">Prompts in Real-World Environments:</font></h4>
                    <ul class="list-inline">
                        <li>
                            <a href="data/planner_prompts.txt"><font size="+1">Planner</font></a> | 
                            <a href="data/composer_prompts.txt"><font size="+1">Composer</font></a> |
                            <!-- <a href="data/planner_prompts.txt"><font size="+1">Planner</font></a> | -->
                            <!-- <a href="https://research.google/people/tianli-ding/">Tianli Ding</a><sup>1</sup>,  -->
        
                        </li>
                        <br>
                    </ul>
                </div>
            </div>
            <hr class="section-divider">
            <p>
                This paper has presented a universal Embodied Arti
                ficial Intelligence scheme for robot manipulation tasks,
                which combines the generation ability of large lan
                guage model and the stable advantages of diffusion
                policy to generate robot actions. This scheme improves
                the flexibility of robot action generation, enhances the
                success rate and generalization of task execution. The
                simulation results have demonstrated the effectiveness
                and correctness of this method. In the future, we will
                verify the reliability of the proposed algorithm through
                physical experiments and apply this universal Embodied
                AI solution to humanoid robot manipulation tasks.
            </p>
            <br>
        </div>

        <div class="row">
           <div class="col-md-12">
               <h3>
                   Citation 
               </h3>
               <div class="form-group col-md-10 col-md-offset-1">
                   <div class="form-group col-md-10 col-md-offset-1">
                   <textarea id="bibtex" class="form-control" readonly>
                    @inproceedings{Embodied-AI-with-DP-and-LLM,
                    title={Embodied-AI-with-DP-and-LLM},
                    author={Yue Zhao},
                    booktitle={},
                    year={2024}
                    }</textarea>
                    </div>
               </div>
           </div>
        </div>
    
    </div>
</body>
</html>
