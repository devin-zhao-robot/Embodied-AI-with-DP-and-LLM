<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Embodied-AI-with-DP-and-LLM</title>
    <link rel="stylesheet" href="https://rt-hierarchy.github.io/css/bootstrap.min.css" >
</head>
<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+6">Embodied-AI-with-DP-and-LLM</font></strong> </br> 
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center authors">
                <ul class="list-inline">
                <br>
                <li>
                    <a href="https://github.com/yue-zhao-robot?tab=repositories">Yue Zhao</a><sup>1,2,3</sup>, 
                    <!-- <a href="https://research.google/people/tianli-ding/">Tianli Ding</a><sup>1</sup>,  -->

                </li>
                <br>
                </ul>
                <sup>1</sup>Innovation Center of Beijing
                Embodied Artificial Intelligence Robot, <sup>2</sup>Beijing Jiaotong University,<sup>3</sup>Johns Hopkins University
            </div>
            <br>
        </div>

        <div class="row">
            <div class="col-md-12">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a src="data/root.pdf" target="_blank">
                        <image src="data/paper.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a src="data/1.jpg" target="_blank">
                        <image src="data/1.jpg" height="160px">
                            <!-- <h4><strong>Paper</strong></h4> -->
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <br>

        <div class="row" style="width: 80%; margin: 0 auto;"> 
            <div class="col-md-12">
                <h3 class="col-md-12 text-center"><strong>Abstract</strong></h3>
                <p>
                    The development of large language model
                    (LLM) and visual language model(VLM) has driven rapid
                    progress in embodied artificial intelligence (Embodied AI)
                    technology, opening up new avenues for robots to interact
                    with the physical world and providing vast space for robots
                    to perform more complex manipulation tasks. The diffusion
                    policy does not rely on the robot dynamics model and can
                    elegantly handle multimodal action distributions, itâ€™s suitable
                    for high-dimensional action spaces, and exhibits impressive
                    training stability, making it highly suitable for robot precise
                    control task. However, the diffusion policy cannot interact
                    with the physical world and is difficult to guarantee the
                    generalization for different task scenarios. To address these
                    issues, this article proposes an Embodied AI scheme based
                    on the LLM and the diffusion policy. This solution firstly
                    converts human natural language instructions into robot
                    manipulation subtasks by LLM and then generates robot
                    actions based on the diffusion policy. Compared to existing
                    Embodied AI solutions, the novelty of our method lies in two
                    aspects: even when facing complex human instructions, it only
                    needs to call the LLM once, reducing the time consumption
                    of frequently calling the LLM. And, we increase the accuracy
                    of task execution by suppressing the states of objects that
                    we are not interested in while enhancing the states of task
                    objects to guide the specific behavioral actions of robots. To
                    validate our idea, we conducted simulation experiments on a
                    7 dof Franka Emika Panda using Isaac Sim. The experimental
                    results show the correctness and generalization of our method.
                </p>
            </div>
        </div>

        <br>

        <h3 class="col-md-12 text-center"><strong>Video</strong></h3>

        <!-- <p style="text-align:center; width: 80%; margin: 0 auto">
            <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                <source src="data/Screencast.webm" type="video/webm">
                Your browser does not support the video tag.
            </video>
        </p> -->


        <div style="width: 100%; margin: 0 auto">
            <div style="width: 20%; float: left; text-align:center; margin: 0 auto">
                <!-- <a src="data/method0.jpg" target="_blank"> -->
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                        <source src="data/2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </a>
            </div>
            <div style="width: 20%; float: right; text-align:center; margin: 0 auto">
                <!-- <a src="data/method1.png" target="_blank"> -->
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                        <source src="data/2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </a>
            </div>
            <div style="width: 20%; float: right; text-align:center; margin: 0 auto">
                <!-- <a src="data/method1.png" target="_blank"> -->
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                        <source src="data/2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </a>
            </div>
            <div style="width: 20%; float: right; text-align:center; margin: 0 auto">
                <!-- <a src="data/method1.png" target="_blank"> -->
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                        <source src="data/2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </a>
            </div>
            <div style="width: 20%; float: right; text-align:center; margin: 0 auto">
                <!-- <a src="data/method1.png" target="_blank"> -->
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                        <source src="data/2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </a>
            </div>
        </div>

        <br>
        <hr class="section-divider">

        <div class="row">
            <div class="col-md-12">
                <h3 class="col-md-12 text-center"><strong>Method</strong></h3>
                <p>
                    The implementation process of our Embodied AI scheme is shown in Algorithm 1.
                </p>
                <div style="width: 70%; margin: 0 auto">
                    <div style="width: 40%; float: left; text-align:center; margin: 0 auto">
                        <!-- <a src="data/method0.jpg" target="_blank"> -->
                            <figure>
                                <img src="data/2.jpg" class="img-responsive">
                            </figure>
                        </a>
                        <!-- <figcaption>XXXXXX</figcaption> -->
                    </div>
                    <div style="width: 40%; float: right; text-align:center; margin: 0 auto">
                        <!-- <a src="data/method1.png" target="_blank"> -->
                            <figure>
                                <img src="data/2.png" class="img-responsive">
                            </figure>
                        </a>
                        <!-- <figcaption>XXXXXXX</figcaption> -->
                    </div>
                </div>
                <br>
                <br>
                <br>
                <br>
                <p class = "text-center">In Algorithm 1, based on the environmental informa-
                    tion collected by the RGB-D depth camera, LLM is first
                    used to decompose human natural language instructions
                    into several sub-tasks that can be executed by robots
                    according to the designed prompt template. Then, a
                    dataset combined states with actions is obtained in
                    NVIDIA Isaac Sim. Based on this dataset, a diffusion
                    model is trained to approximate the trajectory distri-
                    bution. Then, based on the diffusion model, a reverse
                    denoising process is carried out, ultimately obtaining the
                    robot trajectory action..</p>

                <br>
                <!-- <figcaption> <b>Left:</b>XXXXXXXXXXX
                </figcaption> -->
                <br>
                <!-- We leverage a single VLM for both queries based on RT-2 that encapsulate the broad prior knowledge in internet-scale data at each level of the action hierarchy. -->
                <!-- <figcaption><b>Right:</b> XXXXXXXXXXX
                </figcaption> -->
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <h3 class="col-md-12 text-center"><strong>Experiments</strong></h3>
                <p>We evaluate XXXXXX</p>
                <!-- Training -->
                <br>
                <h4 class="text-center"><strong>Training on Diverse Tasks</strong></h4>
                <br>
                <a src="data/method0.jpg" target="_blank">
                    <figure>
                        <img src="data/method0.jpg" class="img-responsive">
                        
                    </figure>
                </a>
                <figcaption>Fig. 3 - XXXXXXXXXXX
                </figcaption>
                <br>
                <p> <b>XXXXXXXXXXXXXXX</b>, XXXXXXXXXXXX </p>
            </div>
        </div>

        <hr class="section-divider">

        <div class="row col-md-12">

            <!--  -->
            <h4 class="text-center"><strong>Generalization</strong></h4>
            <p>Next, we test the ability of XXXXXXXXXX</p>
            <br>

            <h4 class="text-center"><strong>New Scenes</strong></h4>
            <br>
            <div style="width: 80%; margin: 0 auto;">
                <a src="data/method0.jpg" target="_blank">
                    <figure>
                        <img src="data/method0.jpg" class="img-responsive">
                        
                    </figure>
                </a>
                <figcaption>XXXXXXXXXXXXXXXX
                </figcaption>
            </div>
            <br>
            <p>
                XXXXXXXXXXX</b>.
            </p>
            <br>

            <h4 class="text-center"><strong>New Objects</strong></h4>
            <br>
            <div style="width: 80%; margin: 0 auto;">
                <a src="data/method0.jpg" target="_blank" style="width: 80%;">
                    <figure>
                        <img src="data/method0.jpg" class="img-responsive">
                    </figure>
                </a>
                <figcaption>XXXXXXXXXXX
                </figcaption>
            </div>
            <br>
            <div style="width: 80%; margin: 0 auto;">
                <a href="data/method0.jpg" target="_blank" style="width: 80%;">
                    <figure>
                        <img src="data/method0.jpg" class="img-responsive">
                    </figure>
                </a>
                <figcaption>XXXXXXXXX
                </figcaption>
            </div>
            <br>
            <p>
                We see that Embodied-AI-with-DP-and-LLM <b>generalizes XXXXXXX
            </p>
            <p>
                This paper has presented a universal Embodied Arti-
                ficial Intelligence scheme for robot manipulation tasks,
                which combines the generation ability of large lan-
                guage model and the stable advantages of diffusion
                policy to generate robot actions. This scheme improves
                the flexibility of robot action generation, enhances the
                success rate and generalization of task execution. The
                simulation results have demonstrated the effectiveness
                and correctness of this method. In the future, we will
                verify the reliability of the proposed algorithm through
                physical experiments and apply this universal Embodied
                AI solution to humanoid robot manipulation tasks.
            </p>
            <br>
        </div>

        <div class="row">
           <div class="col-md-12">
               <h3>
                   Citation 
               </h3>
               <div class="form-group col-md-10 col-md-offset-1">
                   <div class="form-group col-md-10 col-md-offset-1">
                   <textarea id="bibtex" class="form-control" readonly>
                    @inproceedings{Embodied-AI-with-DP-and-LLM,
                    title={Embodied-AI-with-DP-and-LLM},
                    author={Yue Zhao},
                    booktitle={},
                    year={2024}
                    }</textarea>
                    </div>
               </div>
           </div>
        </div>
    
    </div>
</body>
</html>
