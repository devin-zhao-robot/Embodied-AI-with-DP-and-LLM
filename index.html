<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Embodied-AI-with-DP-and-LLM</title>
    <link rel="stylesheet" href="https://rt-hierarchy.github.io/css/bootstrap.min.css" >
</head>
<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+6">Embodied-AI-with-DP-and-LLM</font></strong> </br> 
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center authors">
                <ul class="list-inline">
                <br>
                <li>
                    <a href="https://github.com/yue-zhao-robot?tab=repositories">Yue Zhao</a><sup>1,2</sup>, 
                    <!-- <a href="https://research.google/people/tianli-ding/">Tianli Ding</a><sup>1</sup>,  -->

                </li>
                <br>
                </ul>
                <sup>1</sup>X-Humanoid, <sup>2</sup>Johns Hopkins University
            </div>
            <br>
        </div>

        <div class="row">
            <div class="col-md-12">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a src="data/ICRA2025.pdf" target="_blank">
                        <image src="data/paper.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a src="data/above-abstract.png" target="_blank">
                        <image src="data/above-abstract.png" height="160px">
                            <!-- <h4><strong>Paper</strong></h4> -->
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <br>

        <div class="row" style="width: 80%; margin: 0 auto;"> 
            <div class="col-md-12">
                <h3 class="col-md-12 text-center"><strong>Abstract</strong></h3>
                <p>
                    The development of large language model (LLM) and visual language model(VLM) has driven rapid progress in embodied artificial intelligence (Embodied AI) technology,
                    opening up new avenues for robots to interact with the
                    physical world and providing vast space for robots to perform
                    more complex manipulation tasks. Traditional robot trajectory
                    planning and control methods are sensitive to disturbances and
                    have strict hardware requirements, making them difficult to
                    apply in precise robot manipulation tasks. The diffusion policy
                    does not rely on the robot dynamics model and can elegantly
                    handle multimodal action distributions, it's suitable for high-
                    dimensional action spaces, and exhibits impressive training
                    stability, making it highly suitable for robot precise control
                    task. However, the diffusion policy cannot interact with the
                    physical world and is difficult to guarantee the generalization
                    for different task scenarios. To address these issues, this article
                    proposes an Embodied AI scheme based on the LLM and the
                    diffusion policy. This solution firstly converts human natural
                    language instructions into robot manipulation subtasks by
                    LLM and then generates robot actions based on the diffusion
                    policy. The combination of LLM and diffusion policy addresses
                    the existing problems in robot manipulation tasks, this method
                    not only inherits the advantages of efficient and stable diffusion
                    models, but also realizes the interaction between robots and the
                    physical world, and has strong generalization to different tasks.
                    To validate our idea, we conducted simulation experiments on a
                    7 dof Franka Emika Panda using Isaac Sim. The experimental
                    results show that compared to previous embodied intelligence
                    schemes, the scheme designed in this paper greatly improves
                    the success rate and safety of task execution. In addition,
                    this scheme has strong generalization ability for different task
                    scenarios.
                </p>
            </div>
        </div>

        <br>

        <h3 class="col-md-12 text-center"><strong>Video</strong></h3>

        <p style="text-align:center; width: 80%; margin: 0 auto">
            <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                <source src="data/Screencast.webm" type="video/webm">
                Your browser does not support the video tag.
            </video>
        </p>

        <br>
        <hr class="section-divider">

        <div class="row">
            <div class="col-md-12">
                <h3 class="col-md-12 text-center"><strong>Method</strong></h3>
                <p>
                    Our method XXXXX
                </p>
                <div style="width: 70%; margin: 0 auto">
                    <div style="width: 45%; float: left; text-align:center; margin: 0 auto">
                        <!-- <a src="data/method0.jpg" target="_blank"> -->
                            <figure>
                                <img src="data/method0.jpg" class="img-responsive">
                            </figure>
                        </a>
                        <figcaption>XXXXXX</figcaption>
                    </div>
                    <div style="width: 45%; float: right; text-align:center; margin: 0 auto">
                        <!-- <a src="data/method1.png" target="_blank"> -->
                            <figure>
                                <img src="data/method1.png" class="img-responsive">
                            </figure>
                        </a>
                        <figcaption>XXXXXXX</figcaption>
                    </div>
                </div>
                <figcaption class = "text-center">Embodied-AI-with-DP-and-LLM Overview,XXXXXXX.</figcaption>

                <br>
                <figcaption> <b>Left:</b>XXXXXXXXXXX
                </figcaption>
                <br>
                <!-- We leverage a single VLM for both queries based on RT-2 that encapsulate the broad prior knowledge in internet-scale data at each level of the action hierarchy. -->
                <figcaption><b>Right:</b> XXXXXXXXXXX
                </figcaption>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <h3 class="col-md-12 text-center"><strong>Experiments</strong></h3>
                <p>We evaluate XXXXXX</p>
                <!-- Training -->
                <br>
                <h4 class="text-center"><strong>Training on Diverse Tasks</strong></h4>
                <br>
                <a src="data/method0.jpg" target="_blank">
                    <figure>
                        <img src="data/method0.jpg" class="img-responsive">
                        
                    </figure>
                </a>
                <figcaption>Fig. 3 - XXXXXXXXXXX
                </figcaption>
                <br>
                <p> <b>XXXXXXXXXXXXXXX</b>, XXXXXXXXXXXX </p>
            </div>
        </div>

        <hr class="section-divider">

        <div class="row col-md-12">

            <!--  -->
            <h4 class="bold-center"><strong>Generalization</strong></h4>
            <p>Next, we test the ability of XXXXXXXXXX</p>
            <br>

            <h4 class="text-center"><strong>New Scenes</strong></h4>
            <br>
            <a src="data/method0.jpg" target="_blank">
                <figure>
                    <img src="data/method0.jpg" class="img-responsive">
                    
                </figure>
            </a>
            <figcaption>XXXXXXXXXXXXXXXX
            </figcaption>
            <br>
            <p>
                XXXXXXXXXXX</b>.
            </p>
            <br>

            <h4 class="text-center"><strong>New Objects</strong></h4>
            <br>
            <div style="width: 80%; margin: 0 auto;">
            <a src="data/method0.jpg" target="_blank" style="width: 80%;">
                <figure>
                    <img src="data/method0.jpg" class="img-responsive">
                </figure>
            </a>
            <figcaption>XXXXXXXXXXX
            </figcaption>
            </div>
            <br>
            <div style="width: 80%; margin: 0 auto;">
            <a href="data/method0.jpg" target="_blank" style="width: 80%;">
                <figure>
                    <img src="data/method0.jpg" class="img-responsive">
                </figure>
            </a>
            <figcaption>XXXXXXXXX
            </figcaption>
            </div>
            <br>
            <p>
                We see that Embodied-AI-with-DP-and-LLM <b>generalizes XXXXXXX
            </p>
            <br>
        </div>

        <div class="row">
           <div class="col-md-12">
               <h3>
                   Citation 
               </h3>
               <div class="form-group col-md-10 col-md-offset-1">
                   <div class="form-group col-md-10 col-md-offset-1">
                   <textarea id="bibtex" class="form-control" readonly>
                    @inproceedings{Embodied-AI-with-DP-and-LLM,
                    title={Embodied-AI-with-DP-and-LLM},
                    author={Yue Zhao},
                    booktitle={},
                    year={2024}
                    }</textarea>
                    </div>
               </div>
           </div>
        </div>
    
    </div>
</body>
</html>
